{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d5cf41-88c0-4bd7-ae8a-bcf1ca46c45d",
   "metadata": {},
   "source": [
    "# Convert MIDI piece to PIG format\n",
    "\n",
    "Some relevant constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50f246e5-fb89-4b12-97e3-5a99134ba7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pitch_to_int_mapping: {'A1': 0, 'A2': 1, 'A3': 2, 'A4': 3, 'A5': 4, 'A6': 5, 'B1': 6, 'B2': 7, 'B3': 8, 'B4': 9, 'B5': 10, 'B6': 11, 'Bb1': 12, 'Bb2': 13, 'Bb3': 14, 'Bb4': 15, 'Bb5': 16, 'Bb6': 17, 'C#1': 18, 'C#2': 19, 'C#3': 20, 'C#4': 21, 'C#5': 22, 'C#6': 23, 'C#7': 24, 'C1': 25, 'C2': 26, 'C3': 27, 'C4': 28, 'C5': 29, 'C6': 30, 'C7': 31, 'D1': 32, 'D2': 33, 'D3': 34, 'D4': 35, 'D5': 36, 'D6': 37, 'D7': 38, 'E1': 39, 'E2': 40, 'E3': 41, 'E4': 42, 'E5': 43, 'E6': 44, 'E7': 45, 'Eb1': 46, 'Eb2': 47, 'Eb3': 48, 'Eb4': 49, 'Eb5': 50, 'Eb6': 51, 'Eb7': 52, 'F#1': 53, 'F#2': 54, 'F#3': 55, 'F#4': 56, 'F#5': 57, 'F#6': 58, 'F#7': 59, 'F1': 60, 'F2': 61, 'F3': 62, 'F4': 63, 'F5': 64, 'F6': 65, 'F7': 66, 'G#1': 67, 'G#2': 68, 'G#3': 69, 'G#4': 70, 'G#5': 71, 'G#6': 72, 'G1': 73, 'G2': 74, 'G3': 75, 'G4': 76, 'G5': 77, 'G6': 78}\n",
      "\n",
      "\n",
      "finger_to_int_mapping: {'-3_-1': 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, '-1': 6, '-4': 7, '-2': 8, '2_3': 9, '3_1': 10, '5_4': 11, '5_3': 12, '1_5': 13, '-5_-4': 14, '-4_-5': 15, '-2_-3': 16, '4_': 17, '3_4': 18, '2': 19, '3_4_5': 20, '-3_-2': 21, '4_5': 22, '3_5': 23, '-4_-3': 24, '4_2': 25, '5': 26, '-2_1': 27, '-1_-3': 28, '-5_-3': 29, '-1_1': 30, '3_2': 31, '-2_-1': 32, '5_1': 33, '5_2': 34, '-5_-1': 35, '1_3': 36, '2_4_5': 37, '4': 38, '-3': 39, '-3_-5': 40, '-2_-5': 41, '1': 42, '2_5': 43, '-1_-5': 44, '-5': 45, '-1_-4': 46, -4: 47, '1_2': 48, '4_3': 49, '4_1': 50, '1_4': 51, '2_4': 52, '-3_-4': 53, '3': 54, -1: 55, '-1_-2': 56, -5: 57, '2_1': 58, -3: 59, -2: 60}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# RELEVANT CONSTANTS\n",
    "PATH_TO_DATASET_FOLDER = './PianoFingeringDataset_v1.2/PianoFingeringDataset_v1.2/FingeringFiles'\n",
    "FINGERING_TYPE_TO_ANALYZE = \"1\"    # there are 8 distinct fingerings done by 8 different people\n",
    "directory_path = PATH_TO_DATASET_FOLDER\n",
    "\n",
    "spelled_pitch_values = set()\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_table(file_path, sep=\"\\t\", skiprows=1, names=[\"noteID\", \"onset_time\", \"offset_time\", \"spelled_pitch\", \"onset_velocity\", \"offset_velocity\", \"channel\", \"finger_number\"])\n",
    "        spelled_pitch_values.update(df['spelled_pitch'].unique())\n",
    "\n",
    "\n",
    "spelled_pitch_values = sorted(spelled_pitch_values)    # convert \"spelled pitch\" field to a number: create the mapping in the first place\n",
    "pitch_to_int_mapping = {p:i for i, p in enumerate(spelled_pitch_values)}\n",
    "\n",
    "print(f\"pitch_to_int_mapping: {pitch_to_int_mapping}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# We need to do the same with the fingerings themselves, since the dataset includes fingerings like \"-1_-2\" to denote finger changes on the same note\n",
    "fingering_map = set()\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_table(file_path, sep=\"\\t\", skiprows=1, names=[\"noteID\", \"onset_time\", \"offset_time\", \"spelled_pitch\", \"onset_velocity\", \"offset_velocity\", \"channel\", \"finger_number\"])\n",
    "        fingering_map.update(df['finger_number'].unique())\n",
    "\n",
    "finger_to_int_mapping = {p:i for i, p in enumerate(fingering_map)}    # convert \"spelled pitch\" field to a number: create the mapping in the first place\n",
    "print(f\"finger_to_int_mapping: {finger_to_int_mapping}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b2d08a7-d7f5-4c85-a220-c9cda6a3ffad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIG-formatted text file saved as output_pig_format.txt\n"
     ]
    }
   ],
   "source": [
    "from music21 import converter, midi, note, chord\n",
    "import pandas as pd\n",
    "\n",
    "def midi_to_pig_format(midi_file, output_txt=\"output_pig_format.csv\"):\n",
    "    # Parse MIDI file\n",
    "    score = converter.parse(midi_file)\n",
    "\n",
    "    # Extract MIDI parts (assume first track is RH, second is LH if multi-track)\n",
    "    parts = score.parts\n",
    "    is_multitrack = len(parts) > 1\n",
    "\n",
    "    if not is_multitrack:\n",
    "        print(\"NOT MULTITRACK!\")\n",
    "        return\n",
    "\n",
    "    note_data = []\n",
    "    note_id = 0  # Sequential numbering\n",
    "\n",
    "    for part_index, part in enumerate(parts):        \n",
    "        # Assign hand based on track index (if multitrack, first track is RH (0), second is LH (1))\n",
    "        hand = 0 if not is_multitrack or part_index == 0 else 1\n",
    "\n",
    "        for element in part.flat.notes:\n",
    "            if isinstance(element, note.Note):  # Single Note\n",
    "                spelled_pitch = element.nameWithOctave\n",
    "                onset_time = element.offset  # Start time in quarter notes\n",
    "                offset_time = onset_time + element.quarterLength  # End time\n",
    "\n",
    "                # MIDI velocity (MIDI files may not always contain explicit velocity info)\n",
    "                onset_velocity = element.volume.velocity or 64  # Default to 64 if missing\n",
    "                offset_velocity = onset_velocity  # No explicit release velocity in MIDI\n",
    "\n",
    "                note_data.append([\n",
    "                    note_id, onset_time, offset_time, spelled_pitch,\n",
    "                    onset_velocity, offset_velocity, hand\n",
    "                ])\n",
    "                note_id += 1\n",
    "\n",
    "            elif isinstance(element, chord.Chord):  # Chord (multiple notes at same time)\n",
    "                for pitch in element.pitches:\n",
    "                    spelled_pitch = pitch.nameWithOctave\n",
    "                    onset_time = element.offset\n",
    "                    offset_time = onset_time + element.quarterLength\n",
    "                    onset_velocity = element.volume.velocity or 64\n",
    "                    offset_velocity = onset_velocity\n",
    "\n",
    "                    note_data.append([\n",
    "                        note_id, onset_time, offset_time, spelled_pitch,\n",
    "                        onset_velocity, offset_velocity, hand\n",
    "                    ])\n",
    "                    note_id += 1\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(note_data, columns=[\n",
    "        \"noteID\", \"onset_time\", \"offset_time\", \"spelled_pitch\",\n",
    "        \"onset_velocity\", \"offset_velocity\", \"channel\"\n",
    "    ])\n",
    "\n",
    "    # Sort by onset time\n",
    "    df = df.sort_values(by=\"onset_time\").reset_index(drop=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    with open(output_txt, \"w\") as f:\n",
    "        # Write header\n",
    "        f.write(\"noteID onset_time offset_time spelled_pitch onset_velocity offset_velocity channel\\n\")\n",
    "        # Write each row\n",
    "        for _, row in df.iterrows():\n",
    "            f.write(f\"{row['noteID']} {row['onset_time']} {row['offset_time']} {row['spelled_pitch']} \"\n",
    "                    f\"{row['onset_velocity']} {row['offset_velocity']} {row['channel']}\\n\")\n",
    "\n",
    "    print(f\"PIG-formatted text file saved as {output_txt}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# CONVERTS MIDI FILE TO CSV\n",
    "midi_file = \"TESTY.mid\"  # Replace with your MIDI file path\n",
    "df_pig = midi_to_pig_format(midi_file, \"output_pig_format.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c685f472-ab3f-485f-b37d-d30b8815e3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noteID onset_time offset_time spelled_pitch onset_velocity offset_velocity channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>395 0.0 3.75 C#3 80 80 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>397 0.0 3.75 A3 80 80 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>396 0.0 3.75 E3 80 80 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0 0.5 1.0 C#5 80 80 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 1.0 1.5 C#5 80 80 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>393 287.5 288.0 A4 80 80 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>394 288.0 291.75 A4 80 80 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>683 288.0 291.75 C#3 80 80 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>684 288.0 291.75 E3 80 80 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>685 288.0 291.75 A3 80 80 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>686 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    noteID onset_time offset_time spelled_pitch onset_velocity offset_velocity channel\n",
       "0                             395 0.0 3.75 C#3 80 80 1                                \n",
       "1                              397 0.0 3.75 A3 80 80 1                                \n",
       "2                              396 0.0 3.75 E3 80 80 1                                \n",
       "3                                0 0.5 1.0 C#5 80 80 0                                \n",
       "4                                1 1.0 1.5 C#5 80 80 0                                \n",
       "..                                                 ...                                \n",
       "681                         393 287.5 288.0 A4 80 80 0                                \n",
       "682                        394 288.0 291.75 A4 80 80 0                                \n",
       "683                       683 288.0 291.75 C#3 80 80 1                                \n",
       "684                        684 288.0 291.75 E3 80 80 1                                \n",
       "685                        685 288.0 291.75 A3 80 80 1                                \n",
       "\n",
       "[686 rows x 1 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"output_pig_format.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a56f261-e0ca-4a21-9c7a-45e40f745323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf468639-70d0-474b-a1ea-dc64c2304e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36856c20-7c6b-4998-8c79-4fb5636d9a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457daeb2-54e8-4950-8554-4a4211898f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab13d04-cdf5-40ac-adb1-beabab34fe68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea6c3aa-c840-47e7-b9b4-16aadf0a712f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcec6958-3c6f-4893-83e0-d9411291811c",
   "metadata": {},
   "source": [
    "# Load the Relevant Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92e71c38-654f-44be-9618-53079a65b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pieces found: 148\n",
      "numeric_data shape: (45976, 5)\n",
      "Numeric scaler mean_: [21.13948002 21.45820221 68.73305638 79.79739429  0.45321472]\n",
      "Numeric scaler var_ : [3.97946937e+02 3.98278724e+02 3.89274639e+02 1.63700132e+01\n",
      " 2.47811138e-01]\n",
      "Total pieces: 148\n",
      "Train pieces: 88\n",
      "Val pieces:   30\n",
      "Test pieces:  30\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "directory_path = './PianoFingeringDataset_v1.2/PianoFingeringDataset_v1.2/FingeringFiles/'\n",
    "\n",
    "all_sequences = []   # will hold raw DataFrames for each piece\n",
    "all_piece_names = []\n",
    "\n",
    "pitch_vocab = set()\n",
    "finger_vocab = set()\n",
    "\n",
    "FINGERING_TYPE_TO_ANALYZE = 1\n",
    "\n",
    "# 1) Gather data by piece\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "    # Example: \"001-01_fingering.txt\" => fingering_label = \"001-01\", rest = \"fingering.txt\"\n",
    "    fingering_label, _ = filename.split('_')  # e.g. \"001-01\"\n",
    "    piece_id, fingering_type = fingering_label.split('-')\n",
    "\n",
    "    # Check if this is a valid file for the desired fingering type\n",
    "    if os.path.isfile(file_path) and FINGERING_TYPE_TO_ANALYZE == int(fingering_type):\n",
    "    # if os.path.isfile(file_path):\n",
    "        df = pd.read_table(\n",
    "            file_path, sep=\"\\t\", skiprows=1,\n",
    "            names=[\n",
    "                \"noteID\", \"onset_time\", \"offset_time\", \"spelled_pitch\",\n",
    "                \"onset_velocity\", \"offset_velocity\", \"channel\",\n",
    "                \"finger_number\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Update global vocab sets\n",
    "        pitch_vocab.update(df[\"spelled_pitch\"].unique())\n",
    "        finger_vocab.update(map(str, df[\"finger_number\"].unique()))\n",
    "\n",
    "        all_piece_names.append(filename)\n",
    "        all_sequences.append(df)\n",
    "\n",
    "# 2) Build the pitch & finger mappings\n",
    "pitch_vocab = sorted(pitch_vocab)\n",
    "pitch_to_idx = {p: i for i, p in enumerate(pitch_vocab)}\n",
    "\n",
    "finger_vocab = sorted(finger_vocab)\n",
    "finger_to_idx = {f: i for i, f in enumerate(finger_vocab)}\n",
    "\n",
    "print(f\"Total number of pieces found: {len(all_sequences)}\")\n",
    "\n",
    "# 3) Convert each piece’s DF into (X_list, y_list) with shape (num_notes, 6) + (num_notes,)\n",
    "raw_encoded_sequences = []  # We'll store the unscaled version first\n",
    "\n",
    "for df in all_sequences:\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for row in df.itertuples(index=False):\n",
    "        spelled_pitch = row.spelled_pitch\n",
    "        finger_str = str(row.finger_number)\n",
    "\n",
    "        # Encode spelled pitch\n",
    "        pitch_int = pitch_to_idx.get(spelled_pitch, 0)  # fallback 0 if unseen\n",
    "\n",
    "        # Encode finger\n",
    "        finger_int = finger_to_idx.get(finger_str, 0)   # fallback 0 if unseen\n",
    "\n",
    "        # Construct feature row:\n",
    "        # [pitch_int, onset_time, offset_time, onset_velocity, offset_velocity, channel]\n",
    "        feature_row = [\n",
    "            pitch_int,\n",
    "            float(row.onset_time),\n",
    "            float(row.offset_time),\n",
    "            float(row.onset_velocity),\n",
    "            float(row.offset_velocity),\n",
    "            float(row.channel)\n",
    "        ]\n",
    "        X_list.append(feature_row)\n",
    "        y_list.append(finger_int)\n",
    "\n",
    "    raw_encoded_sequences.append((X_list, y_list))\n",
    "\n",
    "# 3.5) Gather all numeric features for scaling\n",
    "# We'll do a single pass over raw_encoded_sequences\n",
    "numeric_data = []\n",
    "for (X_list, _) in raw_encoded_sequences:\n",
    "    for row in X_list:\n",
    "        numeric_vals = row[1:]  # 5 numeric features => (onset_time, offset_time, onset_vel, offset_vel, channel)\n",
    "        numeric_data.append(numeric_vals)\n",
    "\n",
    "numeric_data = np.array(numeric_data, dtype=np.float32)\n",
    "print(\"numeric_data shape:\", numeric_data.shape)\n",
    "\n",
    "# 3.6) Fit a StandardScaler on the numeric columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(numeric_data)\n",
    "print(\"Numeric scaler mean_:\", scaler.mean_)\n",
    "print(\"Numeric scaler var_ :\", scaler.var_)\n",
    "\n",
    "# 3.7) Apply the scaler to each piece\n",
    "scaled_sequences = []\n",
    "for (X_list, y_list) in raw_encoded_sequences:\n",
    "    new_X_list = []\n",
    "    for row in X_list:\n",
    "        pitch_val = row[0]\n",
    "        numeric_vals = row[1:]  # shape (5,)\n",
    "        numeric_vals_arr = np.array(numeric_vals, dtype=np.float32).reshape(1, -1)\n",
    "        scaled_vals = scaler.transform(numeric_vals_arr)[0]  # (5,)\n",
    "\n",
    "        # Rebuild the row: pitch_val + scaled numeric\n",
    "        new_row = [pitch_val] + scaled_vals.tolist()\n",
    "        new_X_list.append(new_row)\n",
    "    scaled_sequences.append((new_X_list, y_list))\n",
    "\n",
    "# 4) Split entire set of sequences (by piece) into train/val/test\n",
    "train_val_seqs, test_seqs = train_test_split(\n",
    "    scaled_sequences, test_size=0.2, random_state=42\n",
    ")\n",
    "train_seqs, val_seqs = train_test_split(\n",
    "    train_val_seqs, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Total pieces: {len(scaled_sequences)}\")\n",
    "print(f\"Train pieces: {len(train_seqs)}\")\n",
    "print(f\"Val pieces:   {len(val_seqs)}\")\n",
    "print(f\"Test pieces:  {len(test_seqs)}\")\n",
    "\n",
    "# Each item is now (X, y) with scaled numeric features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac17d3f4-e163-4524-9562-3c72e1330f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiFeatureRNNTagger(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_pitches,     # size of pitch vocab\n",
    "        num_fingers,     # number of finger classes\n",
    "        embed_dim=32,    # dimension for pitch embedding\n",
    "        hidden_dim=64,   # dimension for RNN hidden layer\n",
    "        numeric_dim=5,   # number of extra numeric features\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Embedding for pitch (categorical)\n",
    "        self.embedding_pitch = nn.Embedding(num_pitches, embed_dim)\n",
    "\n",
    "        # 2. A simple linear projection for numeric features => same embed_dim\n",
    "        #    so we can concatenate them with pitch embeddings\n",
    "        self.embedding_numeric = nn.Linear(numeric_dim, embed_dim)\n",
    "\n",
    "        # 3. RNN: input_dim = embed_dim(pitch) + embed_dim(numeric) = 2*embed_dim\n",
    "        self.rnn = nn.RNN(embed_dim * 2, hidden_dim, batch_first=True)\n",
    "\n",
    "        # 4. Final linear layer to map hidden_dim => number of finger classes\n",
    "        self.fc = nn.Linear(hidden_dim, num_fingers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_len, 6)\n",
    "          - x[..., 0]: pitch IDs (long)\n",
    "          - x[..., 1:]: numeric features (float)\n",
    "        returns logits: (batch_size, seq_len, num_fingers)\n",
    "        \"\"\"\n",
    "        # Separate pitch from numeric\n",
    "        pitch_ids = x[..., 0].long()         # shape => (batch_size, seq_len)\n",
    "        numeric_feats = x[..., 1:].float()   # shape => (batch_size, seq_len, 5)\n",
    "\n",
    "        # Embed pitch\n",
    "        pitch_emb = self.embedding_pitch(pitch_ids)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Project numeric\n",
    "        numeric_emb = self.embedding_numeric(numeric_feats)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Concatenate along last dimension => (batch_size, seq_len, 2*embed_dim)\n",
    "        combined_emb = torch.cat((pitch_emb, numeric_emb), dim=2)\n",
    "\n",
    "        # RNN => (batch_size, seq_len, hidden_dim)\n",
    "        rnn_out, _ = self.rnn(combined_emb)\n",
    "\n",
    "        # Final linear => (batch_size, seq_len, num_fingers)\n",
    "        logits = self.fc(rnn_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3acdbe02-6b66-4b34-8e90-6e4de07b4a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_sequence_model(model, train_loader, val_loader, num_epochs=5, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Trains a token-level sequence model on multi-feature input X_seq.\n",
    "    Expects X_seq.shape = (batch_size, seq_len, num_features)\n",
    "           y_seq.shape = (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()  # for token-level classification\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for X_seq, y_seq in train_loader:\n",
    "            # X_seq shape: (batch_size, seq_len, num_features)\n",
    "            # y_seq shape: (batch_size, seq_len)\n",
    "\n",
    "            X_seq = X_seq.to(device, non_blocking=True)\n",
    "            y_seq = y_seq.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: model should output shape (batch_size, seq_len, num_fingers)\n",
    "            logits = model(X_seq)\n",
    "\n",
    "            # Reshape for CrossEntropy:\n",
    "            # Flatten tokens => (batch_size * seq_len, num_fingers)\n",
    "            logits_reshaped = logits.view(-1, logits.size(-1))\n",
    "            y_seq_reshaped = y_seq.view(-1)\n",
    "\n",
    "            loss = criterion(logits_reshaped, y_seq_reshaped)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_acc = evaluate_sequence_model(model, val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | ValAcc: {val_acc:.4f}\")\n",
    "\n",
    "def evaluate_sequence_model(model, data_loader):\n",
    "    \"\"\"\n",
    "    Evaluates token-level accuracy. Expects same shapes as train_sequence_model.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_seq, y_seq in data_loader:\n",
    "            # X_seq: (batch_size, seq_len, num_features)\n",
    "            # y_seq: (batch_size, seq_len)\n",
    "\n",
    "            X_seq = X_seq.to(device, non_blocking=True)\n",
    "            y_seq = y_seq.to(device, non_blocking=True)\n",
    "\n",
    "            logits = model(X_seq)  # => (batch_size, seq_len, num_fingers)\n",
    "            preds = torch.argmax(logits, dim=-1)  # => (batch_size, seq_len)\n",
    "\n",
    "            all_preds.extend(preds.view(-1).cpu().numpy())\n",
    "            all_labels.extend(y_seq.view(-1).cpu().numpy())\n",
    "\n",
    "    return accuracy_score(all_labels, all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1e29452-8739-4f88-a4e2-63451b337d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 2.4655 | ValAcc: 0.3038\n",
      "Epoch 2/10 | Loss: 1.6684 | ValAcc: 0.3596\n",
      "Epoch 3/10 | Loss: 1.5610 | ValAcc: 0.3710\n",
      "Epoch 4/10 | Loss: 1.5155 | ValAcc: 0.3867\n",
      "Epoch 5/10 | Loss: 1.4763 | ValAcc: 0.3976\n",
      "Epoch 6/10 | Loss: 1.4515 | ValAcc: 0.3996\n",
      "Epoch 7/10 | Loss: 1.4261 | ValAcc: 0.3900\n",
      "Epoch 8/10 | Loss: 1.4014 | ValAcc: 0.4269\n",
      "Epoch 9/10 | Loss: 1.3721 | ValAcc: 0.4345\n",
      "Epoch 10/10 | Loss: 1.3413 | ValAcc: 0.4445\n",
      "RNN Test Accuracy = 0.4613\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FingeringDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        \"\"\"\n",
    "        sequences: list of (X_list, y_list),\n",
    "                   where X_list is shape (seq_len, 6)\n",
    "                         y_list is shape (seq_len,)\n",
    "        \"\"\"\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_list, y_list = self.sequences[idx]\n",
    "        # Convert to Tensors\n",
    "        # By now, X_list is already scaled\n",
    "        X_tensor = torch.tensor(X_list, dtype=torch.float32)  # (seq_len, 6)\n",
    "        y_tensor = torch.tensor(y_list, dtype=torch.long)     # (seq_len,)\n",
    "        return X_tensor, y_tensor\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = FingeringDataset(train_seqs)\n",
    "val_dataset   = FingeringDataset(val_seqs)\n",
    "test_dataset  = FingeringDataset(test_seqs)\n",
    "\n",
    "# If your model can handle variable sequence lengths with e.g. a collate_fn,\n",
    "# you can do batch_size > 1. Otherwise, keep batch_size=1.\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=1, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "# Create your model instance\n",
    "rnn_model = MultiFeatureRNNTagger(\n",
    "    num_pitches=len(pitch_vocab),\n",
    "    num_fingers=len(finger_vocab),\n",
    "    embed_dim=32,\n",
    "    hidden_dim=64,\n",
    "    numeric_dim=5\n",
    ")\n",
    "\n",
    "# Train as usual\n",
    "train_sequence_model(rnn_model, train_loader, val_loader, num_epochs=10, lr=1e-3)\n",
    "test_acc_rnn = evaluate_sequence_model(rnn_model, test_loader)\n",
    "print(f\"RNN Test Accuracy = {test_acc_rnn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc482c-e4a1-4f7d-9f20-616b940509ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5423bd4-44b9-4875-a4c8-56a33cfbf2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa05f209-0c22-455b-b7b4-7dd01817d6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b455c-262a-4f67-9be5-372647c3812a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579e850-55bc-4acf-bd1f-3b1c7818691b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2af26-43e3-4738-bf5f-7d797a9fefa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c26efb-269f-484e-8c49-a3ab38a9b664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46c7fd31-17dd-461e-9650-6a8c843b1d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b97b44af-956f-4e4b-9af3-7af2fb13ce48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     noteID  onset_time  offset_time spelled_pitch  onset_velocity  \\\n",
      "0       395         0.0         3.75           C#3              80   \n",
      "1       397         0.0         3.75            A3              80   \n",
      "2       396         0.0         3.75            E3              80   \n",
      "3         0         0.5         1.00           C#5              80   \n",
      "4         1         1.0         1.50           C#5              80   \n",
      "..      ...         ...          ...           ...             ...   \n",
      "681     393       287.5       288.00            A4              80   \n",
      "682     394       288.0       291.75            A4              80   \n",
      "683     683       288.0       291.75           C#3              80   \n",
      "684     684       288.0       291.75            E3              80   \n",
      "685     685       288.0       291.75            A3              80   \n",
      "\n",
      "     offset_velocity  channel  spelled_pitch_int  \n",
      "0                 80        1                 20  \n",
      "1                 80        1                  2  \n",
      "2                 80        1                 41  \n",
      "3                 80        0                 22  \n",
      "4                 80        0                 22  \n",
      "..               ...      ...                ...  \n",
      "681               80        0                  3  \n",
      "682               80        0                  3  \n",
      "683               80        1                 20  \n",
      "684               80        1                 41  \n",
      "685               80        1                  2  \n",
      "\n",
      "[686 rows x 8 columns]\n",
      "\n",
      "=== Multi-Feature RNN Predictions for output_pig_format.txt ===\n",
      "Predicted fingerings (string labels): [16  0  0 23 23 20 23 16  0  0 23 23 20 23 20  0  5  0 29 29 23 20 29 23\n",
      " 20 20 23 20 16 29 16  0  0  9  0 23 29 29 39 29 20 16 39  5  0 23 20 20\n",
      " 16  0  5 29 29 23 20 20 23 20 20 23 20 16  0 20 16 16  0  0 39 29 23 16\n",
      "  0  5 23 20 20  0  9  0 39 39 29 29 23 20 23  0 16  0 23 29  5 23 16  0\n",
      " 23 29 23 29 23 20  0  5  0 29 23  0  0  0 29 20 20 23 29 23 20  0  5 16\n",
      " 23 20 23 23 29 23 20 16  0 35 16 35 23 16  9  0 29 29 20 20  0  5 16 23\n",
      " 20 23  0 16  0 20 29 23 20 16 16  0  0 39 39 39  0  5  0  5 39 20 20 23\n",
      " 16  0  0 23 20 16  9  0  0  0 35  0 35 23 16  9  0 29 29 20 20  0  5 16\n",
      " 23 20 23  5  0 16 20 23 23 20 16  0  5  0 39 39 39  0  5 35 13 16 29 20\n",
      " 39 16  0 39 16 39 16  0  0  0  5  9 23 23 23 23 20 20 20 16  0  0 20 23\n",
      " 20 20 23  0 20  9  9 23 23 20 23  0 23  5 16 23 20 23 20 16  0  0  9  0\n",
      "  9  5 20 23 23 20 20  5  0  0 23  0 20 23 23 20 16 20  0  9 39 39 16 35\n",
      "  0  5 29 23 16  9  0 23 39 39 29 16  0  0 29 23 23 23 16  5  0 23 20 20\n",
      "  0  5 16 20  5  0  0  5 20 20 23 20 20  9  5  0 23 16 20 20 23 20  9 20\n",
      "  9  0  0  0  0  0  5  0 29 39 23 16  0  5 23 20 20 16  0  0 39 39 20 20\n",
      " 20 20 23  0 20 16  5 20 23 16  5  0 23 20 23 39 23 20  0  5  0 20 23  5\n",
      "  0  0 23 20 20 20 20 23 20  5 16  0 23 20 23 39 20 23 20  0  5 20 16  0\n",
      "  0  0 23 39 23 20  5  0  0 39 23 20  0  5 16 23 20 20  5 16 20  5 20 23\n",
      " 20 16  0  0  5 39 39 39 20 13  5  0  5 20 23 23 16  0 23  5 20  9  0  0\n",
      "  0  0  0 39 39 23 13  0 39  9 39 20 20 16  5  0 23 20 23 16  5 20  5 20\n",
      " 23 20  0  5  0  5 39 39 39  5 39  0  0  0 20 23 39 16  0  0 39 39 16  0\n",
      "  0  0  5  0 23 23 20 20 20 20 20  9  5  0 20 20 20 20 20  0  5  0 20 20\n",
      " 23 20 23 20  5  0  0 20 20 23 20 23 20 20  5  0  5 23 23 20 23 20 20 39\n",
      " 20 39 20 39 20 20 20  0  5  0 20 20 23 20 23 20  5  0  5 20 39 39 39 20\n",
      " 39 39 39 16  5  0 39 39 39 20 20 16  5  0 39 39 20 39 39 20 20 39 39 39\n",
      " 39 39 39 39  5  5  9 20 39 20 20 20 20 20  0  5  0 23 20 20 20 20 20 20\n",
      "  0  0  0 20 20  0  0  0 23 23 20 39 39 20 20  0  5  9 20 20 20 39 20 20\n",
      " 20  5  0  0 23 20 20 20 20 20 20  5  0 20  5 20  5  5  0 20 23 39 20 20\n",
      " 20 20 39  9  5  0 20 20 20 20 20 16  0  0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def predict_piece_fingerings_multi(\n",
    "    model,\n",
    "    file_path,\n",
    "    pitch_to_idx,         # dict mapping spelled pitches -> int\n",
    "    scaler=None,          # the numeric scaler fit during training\n",
    "    finger_to_idx=None,   # dict mapping finger labels -> int (optional)\n",
    "    idx_to_finger=None,   # reverse mapping to decode predictions (optional)\n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict fingerings for a single piece using a trained multi-feature RNN/LSTM/Transformer model.\n",
    "    Applies the same numeric scaling used during training if `scaler` is provided.\n",
    "\n",
    "    Returns:\n",
    "      - preds:        (seq_len,) integer array of predicted finger indices\n",
    "      - y_true:       (seq_len,) integer array of actual finger indices (or None if finger_to_idx not given)\n",
    "      - y_pred_labels: decoded predicted finger labels (list of str) or the raw preds\n",
    "      - y_true_labels: decoded actual finger labels (list of str) or None\n",
    "      - acc:          float, accuracy on this piece if y_true is available, else None\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load piece data\n",
    "    df = pd.read_table(\n",
    "        file_path,\n",
    "        sep=\" \",\n",
    "        skiprows=1, #noteID onset_time offset_time spelled_pitch onset_velocity offset_velocity channel\n",
    "        names=[\n",
    "            \"noteID\",\n",
    "            \"onset_time\",\n",
    "            \"offset_time\",\n",
    "            \"spelled_pitch\",\n",
    "            \"onset_velocity\",\n",
    "            \"offset_velocity\",\n",
    "            \"channel\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # 2. Encode spelled pitch -> int\n",
    "    df[\"spelled_pitch_int\"] = df[\"spelled_pitch\"].map(pitch_to_idx).fillna(0).astype(int)\n",
    "\n",
    "  \n",
    "    # 4. Construct multi-feature array X of shape (seq_len, 6)\n",
    "    #    X[..., 0] = spelled_pitch_int (categorical)\n",
    "    #    X[..., 1] = onset_time\n",
    "    #    X[..., 2] = offset_time\n",
    "    #    X[..., 3] = onset_velocity\n",
    "    #    X[..., 4] = offset_velocity\n",
    "    #    X[..., 5] = channel\n",
    "    seq_len = len(df)\n",
    "    X_array = np.zeros((seq_len, 6), dtype=np.float32)\n",
    "    print(df)\n",
    "\n",
    "    # Fill in pitch as int (col 0).\n",
    "    # We'll keep it as float32 for consistency, then convert to long in the model if needed.\n",
    "    X_array[:, 0] = df[\"spelled_pitch_int\"].astype(float)\n",
    "\n",
    "    # Fill in numeric features (cols 1..5)\n",
    "    X_array[:, 1] = df[\"onset_time\"].astype(float)\n",
    "    X_array[:, 2] = df[\"offset_time\"].astype(float)\n",
    "    X_array[:, 3] = df[\"onset_velocity\"].astype(float)\n",
    "    X_array[:, 4] = df[\"offset_velocity\"].astype(float)\n",
    "    X_array[:, 5] = df[\"channel\"].astype(float)\n",
    "\n",
    "    # 5. Apply the same scaler as training (if provided) to columns 1..5\n",
    "    if scaler is not None:\n",
    "        pitch_column = X_array[:, 0].copy()  # store pitch in a temp var\n",
    "        numeric_vals = X_array[:, 1:]       # shape => (seq_len, 5)\n",
    "\n",
    "        # Scale numeric features => shape (seq_len, 5)\n",
    "        numeric_vals_scaled = scaler.transform(numeric_vals)\n",
    "\n",
    "        # Rebuild X_array with pitch + scaled numeric\n",
    "        X_array = np.concatenate([\n",
    "            pitch_column.reshape(-1, 1),  # shape => (seq_len, 1)\n",
    "            numeric_vals_scaled\n",
    "        ], axis=1)  # => shape => (seq_len, 6)\n",
    "\n",
    "    # 6. Convert to Torch tensor: shape => (1, seq_len, 6)\n",
    "    X_tensor = torch.tensor(X_array, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # 7. Move model & input to device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_tensor = X_tensor.to(device)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 8. Inference: model output => (1, seq_len, num_fingers)\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_tensor)\n",
    "        preds = torch.argmax(logits, dim=-1).squeeze(0).cpu().numpy()  # => (seq_len,)\n",
    "\n",
    "    # 9. Decode predictions (optional)\n",
    "    y_pred_labels = preds\n",
    "\n",
    "    # 10. Compute accuracy if y_true is available\n",
    "    acc = 0\n",
    "\n",
    "    return preds, 0, y_pred_labels, 0, 0\n",
    "\n",
    "\n",
    "piece_file = \"output_pig_format.txt\"\n",
    "file_path = piece_file\n",
    "\n",
    "# Suppose your pitch_to_idx, finger_to_idx, idx_to_finger are defined\n",
    "# and your model is a MultiFeatureTransformerTagger or MultiFeatureRNNTagger, etc.\n",
    "\n",
    "# ***IMPORTANT***: Use the SAME scaler you used in training.\n",
    "# e.g. from the step: `scaler = StandardScaler(); scaler.fit(...)`\n",
    "# or you saved it in joblib/pickle.\n",
    "\n",
    "rnn_preds, rnn_true, rnn_pred_labels, rnn_true_labels, rnn_acc = predict_piece_fingerings_multi(\n",
    "    model=rnn_model,\n",
    "    file_path=file_path,\n",
    "    pitch_to_idx=pitch_to_idx,\n",
    "    scaler=scaler,  # pass the trained scaler\n",
    "    finger_to_idx=finger_to_idx,\n",
    "    idx_to_finger=None\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Multi-Feature RNN Predictions for {piece_file} ===\")\n",
    "print(\"Predicted fingerings (string labels):\", rnn_pred_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca2bd42-eb16-4d73-ae89-2afc3ad63412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1cc8b-afe5-4a62-bcea-092e1f807746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5176b81-ed10-4443-bbb2-4e79a0e03131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTED LABELS INCLUDE 0s, WHICH I THINK ARE RESTS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91521c6a-a7a0-492d-b3d7-58dde49eb604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd7470-bf10-47fe-a29f-71bd3465b5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba4d49-8543-4d92-bbf8-00c277cbec1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1578a4f1-0a82-467d-b3df-548f2b9c2bdd",
   "metadata": {},
   "source": [
    "# Convert any MIDI into a musicXML that can be opened in MuseScore\n",
    "\n",
    "Replace MIDI with the OG midi you used before the text file path\n",
    "\n",
    "Then use the list of fingerings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1d9cd36c-ff6d-4386-bd77-829e73b011ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 196 unused fingerings.\n",
      "MusicXML file saved as: output.musicxml\n"
     ]
    }
   ],
   "source": [
    "from music21 import converter, note, chord, articulations\n",
    "\n",
    "def add_fingerings_to_midi(midi_file, fingerings, output_xml=\"output.musicxml\"):\n",
    "    # Parse the MIDI file into a music21 stream\n",
    "    score = converter.parse(midi_file)\n",
    "    \n",
    "    # Flatten the score to iterate over all notes/chords in order\n",
    "    events = score.flat.getElementsByClass([note.Note, chord.Chord])\n",
    "    fingering_index = 0\n",
    "\n",
    "    # Iterate and attach fingerings\n",
    "    for event in events:\n",
    "        if fingering_index >= len(fingerings):\n",
    "            # If there are more notes than fingerings, stop assigning\n",
    "            break\n",
    "\n",
    "        fingering_number = fingerings[fingering_index]\n",
    "\n",
    "        if isinstance(event, note.Note):\n",
    "            # Add fingering articulation to a single note\n",
    "            event.articulations.append(articulations.Fingering(fingering_number))\n",
    "        \n",
    "        elif isinstance(event, chord.Chord):\n",
    "            # Assign the fingering to the lowest note (as is common in MusicXML)\n",
    "            event.notes[0].articulations.append(articulations.Fingering(fingering_number))\n",
    "\n",
    "        fingering_index += 1\n",
    "\n",
    "    # Optionally, warn if not all fingerings were used\n",
    "    if fingering_index < len(fingerings):\n",
    "        print(f\"Warning: {len(fingerings) - fingering_index} unused fingerings.\")\n",
    "    elif fingering_index > len(fingerings):\n",
    "        print(f\"Warning: Some notes/chords have no assigned fingering.\")\n",
    "\n",
    "    # Write the score to a MusicXML file that MuseScore can open\n",
    "    score.write(\"musicxml\", fp=output_xml)\n",
    "    print(f\"MusicXML file saved as: {output_xml}\")\n",
    "\n",
    "# Example usage:\n",
    "midi_file = \"TESTY.mid\"  # Replace with your MIDI file path\n",
    "# Replace with your fingering list (each number corresponds to a note/chord in order)\n",
    "fingerings = rnn_pred_labels\n",
    "add_fingerings_to_midi(midi_file, fingerings, output_xml=\"output.musicxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29b8a7c5-6802-42f7-92c5-9560d4679f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QSocketNotifier: Can only be used with threads started with QThread\n",
      "Loading soundfont: /usr/share/mscore-2.3/sound/sf3/MuseScore_General.sf3\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "Info at line 24 col 33: skipping 'instrument-abbreviation'\n",
      "Info at line 36 col 33: skipping 'instrument-abbreviation'\n",
      "qt.qpa.wayland: Wayland does not support QWindow::requestActivate()\n",
      "Creating interface for ScoreView object\n",
      "SoundFont(/usr/share/mscore-2.3/sound/sf3/MuseScore_General.sf3) Sample(Piano MF B1(L)) start(0) startloop(328404) endloop(478112) end(478118) smaller than SoundFont 2.04 spec chapter 7.10 recommendation\n",
      "SoundFont(/usr/share/mscore-2.3/sound/sf3/MuseScore_General.sf3) Sample(Piano MF B1(R)) start(0) startloop(328404) endloop(478112) end(478118) smaller than SoundFont 2.04 spec chapter 7.10 recommendation\n"
     ]
    }
   ],
   "source": [
    "!musescore output.musicxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10175c6-0320-4c5f-b99d-52b7a9e8c610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smol)",
   "language": "python",
   "name": "smol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
